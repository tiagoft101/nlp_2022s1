{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando coleções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import urllib\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = urllib.request.urlopen(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").read().decode()\n",
    "stopwords_en = set(stopwords_list.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "s = ['meu primeiro documento é meu', 'meu segundo documento é meu também']\n",
    "vectorizer = CountVectorizer(ngram_range=(1,4))\n",
    "x = vectorizer.fit_transform(s)\n",
    "print(x)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lidando com coleções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./datasets/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [s for s in df['review']]\n",
    "vectorizer = CountVectorizer(stop_words = stopwords_en)\n",
    "x = vectorizer.fit_transform(reviews)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregando coleções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideia: somar a frequência de cada palavra ao longo de toda a coleção\n",
    "x_ = np.sum(x, axis=0)\n",
    "\n",
    "tuplas = [ (x_[0,vectorizer.vocabulary_[i]], i) for i in vectorizer.vocabulary_.keys() ]\n",
    "tuplas_ordenadas = sorted(tuplas, reverse=True) # reverse=True pede uma ordenação em ordem decrescente\n",
    "palavras = [ t[1] for t in tuplas_ordenadas ]\n",
    "contagens = [ t[0] for t in tuplas_ordenadas ]\n",
    "\n",
    "print(len([c for c in contagens if c < 50]))\n",
    "\n",
    "\n",
    "n_palavras = 60\n",
    "eixo_x = np.arange(n_palavras)\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.bar(eixo_x[0:n_palavras], contagens[0:n_palavras])\n",
    "plt.xticks(eixo_x[0:n_palavras], palavras[0:n_palavras], rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contar palavras ou detectar a presença de palavras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideia: ao inves de somar a frequencia de palavras, verifico:\n",
    "# (a) a palavra vale 1 se está presente no documento\n",
    "# (b) a palavra vale 0 caso contrário\n",
    "\n",
    "reviews = [s for s in df['review']]\n",
    "vectorizer = CountVectorizer(stop_words = stopwords_en, binary=True)\n",
    "x = vectorizer.fit_transform(reviews)\n",
    "\n",
    "x_ = np.mean(x, axis=0)\n",
    "\n",
    "tuplas = [ (x_[0,vectorizer.vocabulary_[i]], i) for i in vectorizer.vocabulary_.keys() ]\n",
    "tuplas_ordenadas = sorted(tuplas, reverse=True) # reverse=True pede uma ordenação em ordem decrescente\n",
    "palavras = [ t[1] for t in tuplas_ordenadas ]\n",
    "contagens = [ t[0] for t in tuplas_ordenadas ]\n",
    "eixo_x = np.arange(len(palavras))\n",
    "n_palavras = 60\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.bar(eixo_x[0:n_palavras], contagens[0:n_palavras])\n",
    "plt.xticks(eixo_x[0:n_palavras], palavras[0:n_palavras], rotation=90)\n",
    "plt.ylabel('P(word)')\n",
    "plt.xlabel('Words')\n",
    "plt.title('Probability of finding a word in a random document from collection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando por grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passo 1: executar vectorizer ao longo de toda a coleção\n",
    "reviews = [s for s in df['review']]\n",
    "vectorizer = CountVectorizer(stop_words = stopwords_en, binary=True)\n",
    "x = vectorizer.fit_transform(reviews)\n",
    "x_ = np.mean(x, axis=0)\n",
    "\n",
    "# Passo 2: escolher somente elementos correspondentes a cada categoria\n",
    "filtro_pos = df['sentiment']=='positive'\n",
    "x_pos = x[filtro_pos,:] # Escolho linhas com sentimento positivo\n",
    "x_pos_ = np.mean(x_pos, axis=0)\n",
    "filtro_neg = df['sentiment']=='negative'\n",
    "x_neg = x[filtro_neg,:] # Escolho linhas com sentimento negativo\n",
    "x_neg_ = np.mean(x_neg, axis=0)\n",
    "\n",
    "# Passo 3: ordenar palavras de acordo com a soma geral. Vou incluir tambem a contagem nos grupos positivo e negativo!\n",
    "tuplas = [ (\\\n",
    "            x_[0,vectorizer.vocabulary_[i]],\\\n",
    "            i,\\\n",
    "            x_pos_[0, vectorizer.vocabulary_[i]],\\\n",
    "            x_neg_[0, vectorizer.vocabulary_[i]]\\\n",
    "            ) for i in vectorizer.vocabulary_.keys() ]\n",
    "tuplas_ordenadas = sorted(tuplas, reverse=True)\n",
    "palavras = [ t[1] for t in tuplas_ordenadas ]\n",
    "contagens_pos = [ t[2] for t in tuplas_ordenadas ]\n",
    "contagens_neg = [ t[3] for t in tuplas_ordenadas ]\n",
    "\n",
    "eixo_x = np.arange(len(palavras))\n",
    "n_palavras = 15\n",
    "\n",
    "plt.figure(figsize=(14,3))\n",
    "plt.bar(eixo_x[0:n_palavras]-0.125, contagens_pos[0:n_palavras], width=0.25, label='positive')\n",
    "plt.bar(eixo_x[0:n_palavras]+0.125, contagens_neg[0:n_palavras],  width=0.25, label='negative')\n",
    "plt.legend()\n",
    "plt.xticks(eixo_x[0:n_palavras], palavras[0:n_palavras], rotation=90)\n",
    "plt.ylabel('P(word | class)')\n",
    "plt.xlabel('Words')\n",
    "plt.title('Probability of finding a word in a random document from known class')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
