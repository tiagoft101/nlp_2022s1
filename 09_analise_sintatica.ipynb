{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramatica = \"\"\"\n",
    "FRASE -> SUJEITO PREDICADO\n",
    "SUJEITO -> ENTIDADE\n",
    "PREDICADO -> V OBJETO\n",
    "OBJETO -> ENTIDADE\n",
    "ENTIDADE -> ART N\n",
    "ART -> 'o' | 'a'\n",
    "N -> 'lobo'\n",
    "N -> 'casa'\n",
    "V -> 'soprou'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 10 productions (start state = FRASE)\n",
      "    FRASE -> SUJEITO PREDICADO\n",
      "    SUJEITO -> ENTIDADE\n",
      "    PREDICADO -> V OBJETO\n",
      "    OBJETO -> ENTIDADE\n",
      "    ENTIDADE -> ART N\n",
      "    ART -> 'o'\n",
      "    ART -> 'a'\n",
      "    N -> 'lobo'\n",
      "    N -> 'casa'\n",
      "    V -> 'soprou'\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "\n",
    "grammar = CFG.fromstring(gramatica)\n",
    "parser = RecursiveDescentParser(grammar, trace=0)\n",
    "print(parser.grammar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  FRASE                         \n",
      "        ____________|________                    \n",
      "       |                 PREDICADO              \n",
      "       |             ________|________           \n",
      "    SUJEITO         |               OBJETO      \n",
      "       |            |                 |          \n",
      "    ENTIDADE        |              ENTIDADE     \n",
      "  _____|______      |         ________|______    \n",
      "ART           N     V       ART              N  \n",
      " |            |     |        |               |   \n",
      " o           lobo soprou     a              casa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p in parser.parse(\"o lobo soprou a casa\".split()):\n",
    " #   print(p)\n",
    "    p.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quem soprou a casa!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'lobo']\n"
     ]
    }
   ],
   "source": [
    "# Pergunta: quem soprou a casa?\n",
    "trees = []\n",
    "for t in parser.parse(\"o lobo soprou a casa\".split()):\n",
    "    trees.append(t)\n",
    "\n",
    "\n",
    "for subtree in trees[0].subtrees(): # Generate all subtrees\n",
    "    if subtree.label()=='SUJEITO':\n",
    "        print(subtree.leaves())\n",
    "\n",
    "# Pergunta: o que o lobo fez?\n",
    "\n",
    "# Pergunta: o que o lobo soprou?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelando ambiguidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 17 productions (start state = FRASE)\n",
      "    FRASE -> SUJEITO PREDICADO [1.0]\n",
      "    SUJEITO -> ENTIDADE [1.0]\n",
      "    PREDICADO -> V OBJETO [0.5]\n",
      "    PREDICADO -> V OBJETO ADV [0.5]\n",
      "    OBJETO -> ENTIDADE [0.5]\n",
      "    OBJETO -> PREP_ART ENTIDADE [0.5]\n",
      "    ADV -> PREP ENTIDADE [1.0]\n",
      "    ENTIDADE -> N [0.25]\n",
      "    ENTIDADE -> PROPESS [0.25]\n",
      "    ENTIDADE -> N ADJ [0.5]\n",
      "    ADJ -> PREP N [1.0]\n",
      "    PROPESS -> 'ele' [1.0]\n",
      "    V -> 'entrou' [1.0]\n",
      "    PREP_ART -> 'na' [1.0]\n",
      "    N -> 'loja' [0.5]\n",
      "    PREP -> 'de' [1.0]\n",
      "    N -> 'calças' [0.5]\n"
     ]
    }
   ],
   "source": [
    "from nltk import PCFG\n",
    "from nltk.parse import InsideChartParser    \n",
    "\n",
    "\n",
    "gramatica = \"\"\"\n",
    "FRASE -> SUJEITO PREDICADO [1]\n",
    "SUJEITO -> ENTIDADE [1]\n",
    "PREDICADO -> V OBJETO [0.5] | V OBJETO ADV [0.5]\n",
    "OBJETO -> ENTIDADE [0.5] | PREP_ART ENTIDADE [0.5]\n",
    "ADV -> PREP ENTIDADE [1]\n",
    "ENTIDADE -> N [0.25] | PROPESS [0.25] | N ADJ [0.5]\n",
    "ADJ -> PREP N [1]\n",
    "PROPESS -> 'ele' [1]\n",
    "V -> 'entrou' [1]\n",
    "PREP_ART -> 'na' [1]\n",
    "N -> 'loja' [0.5]\n",
    "PREP -> 'de' [1]\n",
    "N -> 'calças' [0.5]\n",
    "\"\"\"\n",
    "\n",
    "grammar = PCFG.fromstring(gramatica)\n",
    "parser = InsideChartParser(grammar, trace=0)\n",
    "print(parser.grammar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 FRASE                                       \n",
      "    _______________|_________                                 \n",
      "   |                     PREDICADO                           \n",
      "   |        _________________|_______                         \n",
      "   |       |                       OBJETO                    \n",
      "   |       |        _________________|_______                 \n",
      "SUJEITO    |       |                      ENTIDADE           \n",
      "   |       |       |          _______________|______          \n",
      "ENTIDADE   |       |         |                     ADJ       \n",
      "   |       |       |         |                ______|____     \n",
      "PROPESS    V    PREP_ART     N              PREP         N   \n",
      "   |       |       |         |               |           |    \n",
      "  ele    entrou    na       loja             de        calças\n",
      "\n",
      "Probability: 0.0078125\n",
      "                 FRASE                                     \n",
      "    _______________|________________                        \n",
      "   |                            PREDICADO                  \n",
      "   |        ________________________|___________            \n",
      "SUJEITO    |             OBJETO                ADV         \n",
      "   |       |        _______|________        ____|_____      \n",
      "ENTIDADE   |       |             ENTIDADE  |       ENTIDADE\n",
      "   |       |       |                |      |          |     \n",
      "PROPESS    V    PREP_ART            N     PREP        N    \n",
      "   |       |       |                |      |          |     \n",
      "  ele    entrou    na              loja    de       calças \n",
      "\n",
      "Probability: 0.0009765625\n"
     ]
    }
   ],
   "source": [
    "for p in parser.parse(\"ele entrou na loja de calças\".split()):\n",
    "    p.pretty_print()\n",
    "    print(\"Probability:\", p.prob())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking (ou: parsing com RegEx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partimos de PoS Tags..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o corpus e pré-processar\n",
    "s = open('./datasets/macmorpho-train.txt', 'r', encoding='utf-8').read()\n",
    "s = re.split(r'\\.+_PU', s)\n",
    "s = [s0.strip() for s0 in s]\n",
    "s = [re.split('\\s+', s0) for s0 in s]\n",
    "s = [ [ tuple(re.split('_', w0)) for w0 in p] for p in s]\n",
    "s = [ [ w for w in p if len(w)==2 ] for p in s ] \n",
    "s = [ [ (w[0].lower(), w[1]) for w in p] for p in s ]\n",
    "s = [p for p in s if len(p)>5]      \n",
    "# s[frase][palavra] = (palavra, tipo)\n",
    "#print(s[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import NgramTagger, DefaultTagger\n",
    "taggers = []\n",
    "taggers.append(DefaultTagger('N'))\n",
    "\n",
    "for n in range(3):\n",
    "    taggers.append(NgramTagger(n+1, s, backoff=taggers[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('o', 'ART'), ('porquinho', 'N'), ('fugiu', 'V'), ('para', 'PREP'), ('a', 'ART'), ('fazenda', 'N')]\n"
     ]
    }
   ],
   "source": [
    "pos = taggers[-1].tag(\"o porquinho fugiu para a fazenda\".split())\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            FRASE                                    \n",
      "          ____________________|________                               \n",
      "      SUJEITO                      PREDICADO                         \n",
      "         |                     ________|______________                \n",
      "      ENTIDADE                |        |           ENTIDADE          \n",
      "   ______|__________          |        |        ______|_________      \n",
      "o/ART          porquinho/N fugiu/V para/PREP a/ART          fazenda/N\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "gramatica = ('''\n",
    "    ENTIDADE: {<ART><N>}\n",
    "    PREDICADO: {<V><PREP>?<ENTIDADE>}\n",
    "    SUJEITO: {<ENTIDADE>}\n",
    "    FRASE: {<SUJEITO><PREDICADO>}\n",
    "    ''')\n",
    "parser = nltk.RegexpParser(gramatica)\n",
    "#print(parser)\n",
    "for p in parser.parse(pos):\n",
    "    print(p.pretty_print())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('o', 'ART'), ('rio', 'NPROP'), ('de', 'NPROP'), ('janeiro', 'NPROP'), ('continua', 'V'), ('lindo', 'ADJ')]\n",
      "(ENTIDADE o/ART rio/NPROP)\n",
      "(ENTIDADE de/NPROP)\n",
      "(ENTIDADE janeiro/NPROP)\n",
      "('continua', 'V')\n",
      "('lindo', 'ADJ')\n"
     ]
    }
   ],
   "source": [
    "pos = taggers[-1].tag(\"o rio de janeiro continua lindo\".split())\n",
    "print(pos)\n",
    "gramatica = ('''\n",
    "    ENTIDADE: {<ART>?<NPROP>}\n",
    "    ''')\n",
    "parser = nltk.RegexpParser(gramatica)\n",
    "#print(parser)\n",
    "for p in parser.parse(pos):\n",
    "    print(p)\n",
    "    #if type(p) != tuple:\n",
    "    #    print(p.leaves())\n",
    "    #if type(p)!=\"<class 'tuple'>\" and p.label()==\"ENTIDADE\":\n",
    "    #    print (p.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
