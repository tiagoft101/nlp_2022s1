{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "plt.style.use('default')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import requests\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = set(stopwords_list.decode().splitlines()) \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 18: Redes Neurais ao Longo do Tempo\n",
    "**Objetivo: : ao fim desta aula, o aluno será capaz de aplicar redes neurais recorrentes para agregar informações de documentos ao longo do tempo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/IMDB Dataset.csv')\n",
    "reviews = list(df['review'])\n",
    "\n",
    "labels = np.array([list(df['sentiment'])]).T\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(labels).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1\n",
    "*Objetivo: entender como sequências são representadas no Keras*\n",
    "\n",
    "Um documento pode ser entendido como uma sequência de palavras.\n",
    "\n",
    "1. Se representássemos nossa coleção como sequências de palavras, chegaríamos a uma matriz. Que significaria cada linha e coluna da matriz?\n",
    "\n",
    "1. Como deveríamos lidar com o fato de que documentos podem ter mais ou menos palavras que a dimensão correspondente da matriz?\n",
    "\n",
    "1. Se cada palavra for representada por um vetor de N dimensões, então nossa coleção passaria a ser representada por um tensor, que é essencialmente uma matriz 3D (na verdade, tensores podem ter várias dimensões, mas usaremos somente 3 neste caso). O que significa cada dimensão desse tensor?\n",
    "\n",
    "1. No código de pré-processamento abaixo, o que fazem as funções `tokenizer.texts_to_sequences()` e `pad_sequences()`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(sequences,maxlen=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2\n",
    "*Objetivo: explicar uma rede neural à partir de sua representação gráfica*\n",
    "\n",
    "Nesta aula, usaremos a camada `embedding`. A camada `embedding` relaciona cada palavra a uma posição em um espaço vetorial – como se estivéssemos multiplicando uma representação one-hot encoding das palavras por uma matriz. A diferença entre a matriz e o embedding é que a camada embedding faz essa transformação através de um dicionário cuja chave é a palavra e o conteúdo é a representação vetorial correspondente.\n",
    "\n",
    "Veja o código da implementação `rede_neural_classificar_por_palavra` abaixo. Se precisar, use a documentação do Keras para responder:\n",
    "\n",
    "1. O que a camada `GlobalAveragePooling1D` faz? Em que dimensão (documentos, dimensões latentes ou tempo) ela opera?\n",
    "1. Esta rede é semelhante a qual outra rede que já trabalhamos nesta disciplina?\n",
    "1. O que esperamos encontrar no espaço latente desta rede?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rede_neural_classificar_por_palavra(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(1000, 2, name='projecao')(x)\n",
    "  x = GlobalAveragePooling1D()(x)\n",
    "  y = Dense(2, activation='sigmoid', name='classificador')(x)\n",
    "  return Model(input_layer, y)\n",
    "\n",
    "rede_neural = rede_neural_classificar_por_palavra(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 3\n",
    "*Objetivo: aplicar uma rede neural com camada embedding e interpretar os resultados*\n",
    "\n",
    "Execute o treino e teste da rede neural.\n",
    "1. Os resultados condizem com o que você esperaria encontrar?\n",
    "1. Aumente o número de dimensões do espaço latente. O desempenho da rede muda? Usando `PCA`, analise o espaço latente de mais alta dimensão. Você consegue visualizar diferenças?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=500, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando layer de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = rede_neural.get_layer('projecao').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rede_neural' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2764/2639105604.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Visualização: onde foi parar cada palavra?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mv_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrede_neural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'projecao'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#proj = PCA(n_components=2, perplexity=5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#v = proj.fit_transform(v_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rede_neural' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualização: onde foi parar cada palavra?\n",
    "v_ = rede_neural.get_layer('projecao').get_weights()[0]\n",
    "\n",
    "#proj = PCA(n_components=2, perplexity=5)\n",
    "#v = proj.fit_transform(v_)\n",
    "v = v_\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(v[:,0], v[:,1], s=1, alpha=0.3, c='b')\n",
    "for s in [\"director\", \"actor\", \"bad\", \"good\", \"excellent\", \"plot\", \"worst\", \"terrible\", \"waste\", \"awful\", \"fantastic\"]:\n",
    "    _n = tokenizer.texts_to_sequences([[s]])[0][0]\n",
    "    plt.text(v[_n,0], v[_n,1], s, ha='center')\n",
    "plt.title('Projeção das palavras no espaço latente')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.xlabel('Componente 1')\n",
    "#plt.xlim([-20,20])\n",
    "#plt.ylim([-20,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 4\n",
    "*Objetivo: entender como uma rede recorrente opera no domínio do tempo*\n",
    "\n",
    "1. Uma rede neural recorrente aplica alguma variação da equação:\n",
    "$$\n",
    "y[t] = f( \\[x[t], y[t-1]\\])\n",
    "$$\n",
    "\n",
    "Isso significa que uma de suas entradas é a saída na interação anterior.\n",
    "\n",
    "Desenhe um diagrama de blocos mostrando esse comportamento.\n",
    "\n",
    "1. A rede recorrente opera no domínio do tempo. Trata-se de um tempo \"discreto\", ou seja, existe t=1, t=2, t=3, etc. No caso da frase \"frase de teste\", quantos \"tempos discretos\" devem existir?\n",
    "\n",
    "1. A rede neural recorrente recebe como entradas a entrada de fato (dados) e também a saída que foi gerada na iteração anterior. Por que não é possível usar como entrada a saída da iteração atual?\n",
    "\n",
    "1. Podemos dizer que esse tipo de rede tem algum tipo de \"memória\" quanto ao passado? Como isso se relaciona a modelos do tipo n-grama?\n",
    "\n",
    "1. De acordo com o diagrama de blocos, a rede emite uma saída para cada instante de tempo. No nosso caso, isso significa uma saída para cada palavra da frase. Consulte a documentação do Keras e encontre: como o bloco `SimpleRNN` pode retornar uma única saída para cada documento, isto é, como a dimensão do tempo é eliminada?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "def rede_neural_com_RNN(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(1000, 2, name='projecao')(x)\n",
    "  x = SimpleRNN(2, activation='linear')(x)\n",
    "  y = Dense(2, activation='sigmoid', name='classificador')(x)\n",
    "  return Model(input_layer, y)\n",
    "\n",
    "rede_neural = rede_neural_com_RNN(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 5\n",
    "*Objetivo: avaliar uma rede neural recorrente para o problema de classificação de sentimentos em avaliações de filmes*\n",
    "\n",
    "Avalie a rede neural recorrente presente no notebook. Como ela se comporta em relação ao:\n",
    "\n",
    "1. Desempenho nas métricas usuais\n",
    "1. Propriedades do espaço latente\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=500, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 2: onde foi parar cada palavra?\n",
    "v_ = rede_neural.get_layer('projecao').get_weights()[0]\n",
    "\n",
    "#proj = PCA(n_components=2, perplexity=5)\n",
    "#v = proj.fit_transform(v_)\n",
    "v = v_\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(v[:,0], v[:,1], s=1, alpha=0.3, c='b')\n",
    "for s in [\"director\", \"actor\", \"bad\", \"good\", \"excellent\", \"plot\", \"worst\", \"terrible\", \"waste\", \"awful\", \"fantastic\"]:\n",
    "    _n = tokenizer.texts_to_sequences([[s]])[0][0]\n",
    "    plt.text(v[_n,0], v[_n,1], s, ha='center')\n",
    "plt.title('Projeção das palavras no espaço latente')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.xlabel('Componente 1')\n",
    "#plt.xlim([-20,20])\n",
    "#plt.ylim([-20,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 6\n",
    "*Objetivo: usar embeddings pré-treinados*\n",
    "\n",
    "Fonte: [Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "1. Em nosso espaço latente, criado pela propagação do gradiente no problema de classificação, o que a posição de uma palavra pode nos informar sobre ela?\n",
    "\n",
    "1. Seria possível \"emprestar\" o embedding criado em um outro processo de classificação, e então treinar somente a etapa de classificação em seu problema específico?\n",
    "\n",
    "1. Um espaço latente muito usado hoje é o GloVe. Ele foi criado por uma equipe de Stanford e tem duas propriedades muito interessantes. As duas propriedades estão discutidas no seu [website](https://nlp.stanford.edu/projects/glove/). Quais são essas propriedades e como elas se assemelham ao que já estudamos neste curso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 7\n",
    "*Objetivo: usar e interpretar um classificador que use embeddings pré-treinados de GloVe*\n",
    "\n",
    "Analise e execute a rede neural que usa embeddings GloVe.\n",
    "1. Como ela se compara com as anteriores?\n",
    "1. O que significa o parâmetro `trainable=False` da camada de embedding?\n",
    "1. Como se comporta o espaço latente de GloVe em relação às palavras que usamos? Como devemos interpretar esse espaço latente?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo os embeddings GloVe\n",
    "f = open(\"./datasets/glove.6B.100d.txt\", encoding=\"utf8\")\n",
    "embeddings_index = dict()\n",
    "for line in f:\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        continue\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruindo a matriz de embeddings (para carregar na camada embedding)\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "def rede_neural_com_Glove(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(len(word_index) + 1, 100, name='projecao', weights=[embedding_matrix], trainable=False)(x)\n",
    "  x = SimpleRNN(2, activation='linear')(x)\n",
    "  y = Dense(2, activation='sigmoid', name='classificador')(x)\n",
    "  return Model(input_layer, y)\n",
    "\n",
    "rede_neural = rede_neural_com_Glove(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=500, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 2: onde foi parar cada palavra?\n",
    "v_ = rede_neural.get_layer('projecao').get_weights()[0]\n",
    "\n",
    "proj = PCA(n_components=2)\n",
    "v = proj.fit_transform(v_)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(v[:,0], v[:,1], s=1, alpha=0.01, c='b')\n",
    "for s in [\"director\", \"actor\", \"bad\", \"good\", \"excellent\", \"plot\", \"worst\", \"terrible\", \"waste\", \"awful\", \"fantastic\"]:\n",
    "    _n = tokenizer.texts_to_sequences([[s]])[0][0]\n",
    "    plt.text(v[_n,0], v[_n,1], s, ha='center')\n",
    "plt.title('Projeção das palavras no espaço latente')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.xlabel('Componente 1')\n",
    "#plt.xlim([-20,20])\n",
    "#plt.ylim([-20,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 8\n",
    "*Objetivo: avaliar se a rede recorrente está usando informações de tempo*\n",
    "\n",
    "Você deve ter observado que usar redes recorrentes é bem mais demorado que usar somente `GlobalAveragePooling1D` ou mesmo que usar as matrizes de DF que usamos na aula anterior. Essa desvantagem pode se compensar: é comum encontrarmos argumentos que dizem que as redes recorrentes acabam conseguindo modelar um pouco da relação temporal entre as palavras. Por outro lado, é possível (ainda não verificamos) que a rede neural recorrente tenha convergido para um estado que somente replica a camada `GlobalAveragePooling1D`.\n",
    "\n",
    "Proponha e execute um experimento capaz de verificar se o desempenho da rede neural recorrente, ao menos neste dataset, se deve ao modelamento da relação temporal entre palavras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
