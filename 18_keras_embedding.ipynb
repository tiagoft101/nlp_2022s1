{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "plt.style.use('default')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import requests\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = set(stopwords_list.decode().splitlines()) \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/IMDB Dataset.csv')\n",
    "reviews = list(df['review'])\n",
    "\n",
    "labels = np.array([list(df['sentiment'])]).T\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(labels).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(sequences,maxlen=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uma classificação por palavra!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rede_neural_classificar_por_palavra(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(1000, 2, name='projecao')(x)\n",
    "  x = GlobalAveragePooling1D()(x)\n",
    "  y = Dense(2, activation='sigmoid', name='classificador')(x)\n",
    "  return Model(input_layer, y)\n",
    "\n",
    "rede_neural = rede_neural_classificar_por_palavra(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=500, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando layer de Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = rede_neural.get_layer('projecao').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 2: onde foi parar cada palavra?\n",
    "v_ = rede_neural.get_layer('projecao').get_weights()[0]\n",
    "\n",
    "#proj = TSNE(n_components=2, perplexity=5)\n",
    "#v = proj.fit_transform(v_)\n",
    "v = v_\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(v[:,0], v[:,1], s=1, alpha=0.3, c='b')\n",
    "for s in [\"director\", \"actor\", \"bad\", \"good\", \"excellent\", \"plot\", \"worst\", \"terrible\", \"waste\", \"awful\", \"fantastic\"]:\n",
    "    _n = tokenizer.texts_to_sequences([[s]])[0][0]\n",
    "    plt.text(v[_n,0], v[_n,1], s, ha='center')\n",
    "plt.title('Projeção das palavras no espaço latente')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.xlabel('Componente 1')\n",
    "#plt.xlim([-20,20])\n",
    "#plt.ylim([-20,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinação com redes recorrentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN, GRU\n",
    "def rede_neural_com_RNN(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(1000, 2, name='projecao')(x)\n",
    "  x = SimpleRNN(2, activation='linear')(x)\n",
    "  y = Dense(2, activation='sigmoid', name='classificador')(x)\n",
    "  return Model(input_layer, y)\n",
    "\n",
    "rede_neural = rede_neural_com_RNN(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=500, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 2: onde foi parar cada palavra?\n",
    "v_ = rede_neural.get_layer('projecao').get_weights()[0]\n",
    "\n",
    "#proj = TSNE(n_components=2, perplexity=5)\n",
    "#v = proj.fit_transform(v_)\n",
    "v = v_\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(v[:,0], v[:,1], s=1, alpha=0.3, c='b')\n",
    "for s in [\"director\", \"actor\", \"bad\", \"good\", \"excellent\", \"plot\", \"worst\", \"terrible\", \"waste\", \"awful\", \"fantastic\"]:\n",
    "    _n = tokenizer.texts_to_sequences([[s]])[0][0]\n",
    "    plt.text(v[_n,0], v[_n,1], s, ha='center')\n",
    "plt.title('Projeção das palavras no espaço latente')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.xlabel('Componente 1')\n",
    "#plt.xlim([-20,20])\n",
    "#plt.ylim([-20,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usar embeddings pré-treinados (GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./datasets/glove.6B.100d.txt\", encoding=\"utf8\")\n",
    "embeddings_index = dict()\n",
    "for line in f:\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        continue\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN, GRU\n",
    "def rede_neural_com_Glove(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(len(word_index) + 1, 100, name='projecao', weights=[embedding_matrix], trainable=False)(x)\n",
    "  x = GRU(2, activation='linear')(x)\n",
    "  y = Dense(2, activation='sigmoid', name='classificador')(x)\n",
    "  return Model(input_layer, y)\n",
    "\n",
    "rede_neural = rede_neural_com_Glove(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=500, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização 2: onde foi parar cada palavra?\n",
    "v_ = rede_neural.get_layer('projecao').get_weights()[0]\n",
    "\n",
    "proj = PCA(n_components=2)\n",
    "v = proj.fit_transform(v_)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(v[:,0], v[:,1], s=1, alpha=0.01, c='b')\n",
    "for s in [\"director\", \"actor\", \"bad\", \"good\", \"excellent\", \"plot\", \"worst\", \"terrible\", \"waste\", \"awful\", \"fantastic\"]:\n",
    "    _n = tokenizer.texts_to_sequences([[s]])[0][0]\n",
    "    plt.text(v[_n,0], v[_n,1], s, ha='center')\n",
    "plt.title('Projeção das palavras no espaço latente')\n",
    "plt.ylabel('Componente 2')\n",
    "plt.xlabel('Componente 1')\n",
    "#plt.xlim([-20,20])\n",
    "#plt.ylim([-20,20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
