{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "plt.style.use('default')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, GlobalAveragePooling1D, Flatten, SimpleRNN, GRU, Dropout, LSTM, Bidirectional, Lambda\n",
    "from tensorflow.keras.backend import sum\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1\n",
    "*Objetivo: explicar as etapas de transformações que são realizadas sobre os dados num processo de classificação com rede neural*\n",
    "\n",
    "O código abaixo mostra todas as etapas de um processo de treinamento de uma rede neural para análise de sentimentos.\n",
    "\n",
    "1. Encontre todas as manipulações e transformações que são feitas com o dataset\n",
    "1. Justifique a relevância de cada uma dessas manipulações, ou, em outras palavras: o que aconteceria se retirássemos cada uma das manipulações?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/IMDB Dataset.csv')\n",
    "reviews = list(df['review'])\n",
    "\n",
    "labels = np.array([list(df['sentiment'])]).T\n",
    "ohe = OneHotEncoder()\n",
    "y = ohe.fit_transform(labels).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(sequences,maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rede_neural_com_RNN(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(1000, 2, name='projecao')(x)\n",
    "  x = LSTM(30)(x)\n",
    "  y = Dense(2, activation='softmax', name='classificador')(x)\n",
    "  return Model(input_layer, y)\n",
    "\n",
    "rede_neural = rede_neural_com_RNN(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 2\n",
    "*Objetivo: explicar o conceito de atenção*\n",
    "\n",
    "Em um processo de classificação de texto, é evidente que nem todas as palavras têm a mesma relevância.\n",
    "\n",
    "1. Na frase: \"este filme sobre o lindo mar é terrível\", quais palavras são (a) absolutamente irrelevantes, (b) um pouco relevantes e (c) mais relevantes?\n",
    "1. Na frase: \"este filme sobre o terrível mar é lindo\", quais palavras são (a) absolutamente irrelevantes, (b) um pouco relevantes e (c) mais relevantes?\n",
    "1. Ao decidir sobre a relevância de palavras, a ordem de palavras importa?\n",
    "1. A relevância de uma palavra depende das palavras que a antecedem?\n",
    "1. A relevância de uma palavra depende das palavras que a sucedem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 3\n",
    "*Objetivo: entender a implementação de um processo de atenção*\n",
    "\n",
    "Uma das maneiras de aplicar atenção a uma sequência é multiplicar cada elemento da sequência por um número real, que é alto se aquele elemento recebe mais atenção, e baixo se o elemento não recebe atenção. Após, o resultado pode ser somado. Em outras palavras, *aplicar atenção* significa fazer uma espécie de *média ponderada* das entradas.\n",
    "\n",
    "Na função abaixo, como esse conceito é aplicado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_atencao(par):\n",
    "  estados, atencao = par[0], par[1]\n",
    "  atencao_aplicada = estados * atencao\n",
    "  contexto = sum(atencao_aplicada, axis=1)\n",
    "  return contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 4\n",
    "*Objetivo: explicar como a atenção pode ser calculada em uma rede neural*\n",
    "\n",
    "Analisando a rede neural abaixo, encontre:\n",
    "1. Quais blocos estão presentes nela, mas não estavam no classificador que fizemos no começo desta aula?\n",
    "1. Como o valor da *atenção* é calculado?\n",
    "1. Como a atenção é aplicada?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rede_neural_com_atencao(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(1000, 2, name='projecao')(x)\n",
    "  x = LSTM(30, return_sequences=True)(x)\n",
    "\n",
    "  atencao = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "  contexto = Lambda(aplicar_atencao)( [x, atencao] )\n",
    "\n",
    "  y = Dense(2, activation='softmax', name='classificador')(contexto)\n",
    "  return Model(input_layer, y), Model(input_layer, atencao)\n",
    "\n",
    "rede_neural, atencao = rede_neural_com_atencao(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 5\n",
    "*Objetivo: visualizar as saídas da camada de atenção*\n",
    "\n",
    "Uma informação relevante de uma rede com atenção é o próprio sinal de atenção, pois ele nos informa quais foram as palavras que mais contribuíram para a classificação final.\n",
    "\n",
    "1. Onde definimos a variável `atencao`? Quando a rede `atencao` foi treinada?\n",
    "1. As palavras mais relevantes encontradas pela rede fazem sentido em relação ao problema?\n",
    "1. As palavras mais relevantes são as mesmas em qualquer amostra de texto? Por que?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = atencao.predict(X_test)\n",
    "#print(X_test[0])\n",
    "#print(at[0])\n",
    "words = tokenizer.sequences_to_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words[10].split())\n",
    "len(at[idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=15\n",
    "plt.figure(figsize=(14,2))\n",
    "n_words = len(words[idx].split())\n",
    "at_ = at[idx,-n_words:]\n",
    "plt.plot(at_)\n",
    "plt.xticks(range(n_words), words[idx].split(), rotation=90)\n",
    "plt.xlim([0,90])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 6\n",
    "*Objetivo: usar uma rede de atenção com encoder bi-direcional*\n",
    "\n",
    "Uma propriedade de textos é que, em algumas situações, o significado de palavras depende de palavras que estão mais adiante. Para modelar isso como uma sequência, uma possível solução é (literalmente) inverter o texto, e trabalhar com ele invertido. Nesse caso, precisamos combinar tanto o modelo que vai \"para a frente\" quanto o que vai \"para trás\".\n",
    "\n",
    "No código abaixo:\n",
    "1. Encontre a camada que lê o texto em sentido *direto*\n",
    "1. Encontre a camada que lê o texto em sentido *reverso*\n",
    "1. Encontre a camada que combina as duas informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_atencao(par):\n",
    "  estados, atencao = par[0], par[1]\n",
    "  atencao_aplicada = estados * atencao\n",
    "  contexto = sum(atencao_aplicada, axis=1)\n",
    "  return contexto\n",
    "\n",
    "def rede_neural_com_atencao_bidirecional(input_dims, n_dims_out):\n",
    "  input_layer = Input(shape=(input_dims,))\n",
    "  x = input_layer\n",
    "  x = Embedding(1000, 2, name='projecao')(x)\n",
    "  x_forward = LSTM(30, return_sequences=True)\n",
    "  x_backward = LSTM(30, return_sequences=True, go_backwards=True)\n",
    "  x = Bidirectional(x_forward, backward_layer=x_backward)(x)\n",
    "\n",
    "  atencao = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "  contexto = Lambda(aplicar_atencao)( [x, atencao] )\n",
    "\n",
    "  y = Dense(2, activation='softmax', name='classificador')(contexto)\n",
    "  return Model(input_layer, y), Model(input_layer, atencao)\n",
    "\n",
    "rede_neural, atencao = rede_neural_com_atencao_bidirecional(200, 2)\n",
    "rede_neural.compile(optimizer='adam', loss='mse')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
    "history = rede_neural.fit(X_train, y_train, epochs=50, validation_split=0.2, callbacks=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 7\n",
    "*Objetivo: avaliar a rede neural com encoder bi-direcional*\n",
    "\n",
    "Usando o código abaixo, avalie o desempenho e o sinal de atenção gerado pela nova rede neural.\n",
    "\n",
    "Houve diferenças evidentes em relação ao caso anterior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_est = rede_neural.predict(X_test)\n",
    "print(classification_report(ohe.inverse_transform(y_test), ohe.inverse_transform(y_est)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = atencao.predict(X_test)\n",
    "#print(X_test[0])\n",
    "#print(at[0])\n",
    "words = tokenizer.sequences_to_texts(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words[10].split())\n",
    "len(at[idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=88\n",
    "plt.figure(figsize=(14,2))\n",
    "n_words = len(words[idx].split())\n",
    "at_ = at[idx,-n_words:]\n",
    "plt.plot(at_)\n",
    "plt.xticks(range(n_words), words[idx].split(), rotation=90)\n",
    "plt.xlim([0,90])\n",
    "plt.ylim([0,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
